{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "In this chapter we are going to train few simple classifiers, discover issues specific to the dataset, present several ways to target them in future model developments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "from imblearn.ensemble import EasyEnsembleClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "data_path = \"/afs/cern.ch/user/a/ananiev/cernbox/output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(data_path, \"hgg_features.tsv\"), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all let's look at the event counts, and how the weights are distributed among these events:\n",
    "\n",
    "* **event_num** — number of events per process\n",
    "* **weight_sums** — sum of weights per process\n",
    "* **weight_frac** — fraction of weights per process normalized to the total weight of all events\n",
    "* **weight_per_event** — average weight of event within the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_num</th>\n",
       "      <th>weight_sums</th>\n",
       "      <th>weight_frac</th>\n",
       "      <th>weight_per_event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VBF</th>\n",
       "      <td>10369</td>\n",
       "      <td>9.612055e-05</td>\n",
       "      <td>0.064293</td>\n",
       "      <td>9.269992e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wp</th>\n",
       "      <td>2112</td>\n",
       "      <td>1.702021e-05</td>\n",
       "      <td>0.011384</td>\n",
       "      <td>8.058813e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Z</th>\n",
       "      <td>4571</td>\n",
       "      <td>1.697019e-05</td>\n",
       "      <td>0.011351</td>\n",
       "      <td>3.712578e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gg</th>\n",
       "      <td>26258</td>\n",
       "      <td>1.364896e-03</td>\n",
       "      <td>0.912952</td>\n",
       "      <td>5.198020e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tt</th>\n",
       "      <td>9861</td>\n",
       "      <td>2.851057e-08</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>2.891245e-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     event_num   weight_sums  weight_frac  weight_per_event\n",
       "VBF      10369  9.612055e-05     0.064293      9.269992e-09\n",
       "Wp        2112  1.702021e-05     0.011384      8.058813e-09\n",
       "Z         4571  1.697019e-05     0.011351      3.712578e-09\n",
       "gg       26258  1.364896e-03     0.912952      5.198020e-08\n",
       "tt        9861  2.851057e-08     0.000019      2.891245e-12"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weight_descriptive(data):\n",
    "    res = {}\n",
    "    \n",
    "    event_counts = data[\"label\"].value_counts().sort_values(ascending=False)\n",
    "    res[\"event_num\"] = event_counts\n",
    "    \n",
    "    weight_sums = data.groupby(\"label\")[\"weight\"].sum().sort_values(ascending=False)\n",
    "    res[\"weight_sums\"] = weight_sums\n",
    "    \n",
    "    weight_frac = weight_sums/data[\"weight\"].sum()\n",
    "    res[\"weight_frac\"] = weight_frac\n",
    "    \n",
    "    weight_per_event = weight_sums/event_counts\n",
    "    res[\"weight_per_event\"] = weight_per_event\n",
    "    \n",
    "    return pd.DataFrame(res)\n",
    "weight_descriptive(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize:\n",
    "\n",
    "* **tt** events are not a lot, and they have relatively small weight\n",
    "* **Wp**, **Z** are quite few, but the weight is significant\n",
    "* **VBF** are quite a lot and the weight is mediocre\n",
    "* **gg** are the decent majority, while their weight is quite small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we list features we can use during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['photon_n', 'photon_1lead_pt', 'photon_1lead_eta', 'photon_1lead_phi',\n",
       "       'photon_1lead_E', 'photon_1lead_etcone20', 'photon_2lead_pt',\n",
       "       'photon_2lead_eta', 'photon_2lead_phi', 'photon_2lead_E',\n",
       "       'photon_2lead_etcone20', 'h_mass', 'lep_n', 'lep_pt_min', 'lep_pt_max',\n",
       "       'lep_pt_mean', 'lep_pt_sum', 'lep_pt_std', 'lep_phi_min', 'lep_phi_max',\n",
       "       'lep_phi_mean', 'lep_phi_sum', 'lep_phi_std', 'lep_E_min', 'lep_E_max',\n",
       "       'lep_E_mean', 'lep_E_sum', 'lep_E_std', 'lep_theta_min',\n",
       "       'lep_theta_max', 'lep_theta_mean', 'lep_theta_sum', 'lep_theta_std',\n",
       "       'lep_charge_min', 'lep_charge_max', 'lep_charge_mean', 'lep_charge_sum',\n",
       "       'lep_charge_std', 'lep_z0_min', 'lep_z0_max', 'lep_z0_mean',\n",
       "       'lep_z0_sum', 'lep_z0_std', 'lep_ptcone30_min', 'lep_ptcone30_max',\n",
       "       'lep_ptcone30_mean', 'lep_ptcone30_sum', 'lep_ptcone30_std',\n",
       "       'lep_etcone20_min', 'lep_etcone20_max', 'lep_etcone20_mean',\n",
       "       'lep_etcone20_sum', 'lep_etcone20_std', 'weight', 'met_et', 'met_phi',\n",
       "       'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train baseline model with ensemble of trees in order to identify at least somewhat relevant features to the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def train_test_val_split(dataset, target, test_size, val_size, *args, **kwargs):\n",
    "    shuffle = kwargs.get(\"shuffle\", True)\n",
    "    del kwargs[\"shuffle\"]\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(data.drop(columns=[target]), data[target], *args, test_size=test_size, shuffle=shuffle, **kwargs)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=val_size, shuffle=False, **kwargs)\n",
    "    return (pd.concat([X_train, y_train], axis=\"columns\"),\n",
    "            pd.concat([X_val, y_val], axis=\"columns\"),\n",
    "            pd.concat([X_test, y_test], axis=\"columns\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split dataset into training, validation and test samples. Test sample constitute 20% of the data, while validation sample is 8%. Rest 72% are training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train, d_val, d_test = train_test_val_split(data, \"label\", 0.2, 0.1, shuffle=True, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "class KeepFeatures(TransformerMixin):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X[self.features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "class FillNa(TransformerMixin):\n",
    "    def __init__(self, mapping):\n",
    "        if isinstance(mapping, dict):\n",
    "            self.mapping = mapping\n",
    "            self.value = None\n",
    "        else:\n",
    "            self.value = mapping\n",
    "            self.mapping = None\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        res = X.copy()\n",
    "        if self.mapping is not None:\n",
    "            for col, val in self.mapping.items():\n",
    "                res[col].fillna(val, inplace=True)\n",
    "        if self.value is not None:\n",
    "            res.fillna(self.value, inplace=True)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def pipeline_maker(pipeline, *args,  **kwargs):\n",
    "    def instantiate():\n",
    "        return pipeline(*args, **kwargs)\n",
    "    return instantiate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we choose features describing the event in general (e.g. photon number, lepton number, missing energy), then, since two photons are important features of the event we pick two the most energetic photons and include all the features describing them in detail (e.g. eta, phi, pt, E). Then we include aggregated features for leptons, like sum of phi, sum of energies, sum of theta. Finally, we include `h_mass`.\n",
    "\n",
    "Sometimes there are no leptons in the event, then we impose zero values for all the features describing them.\n",
    "\n",
    "Find the full feature list below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>photon_n</th>\n",
       "      <th>photon_1lead_pt</th>\n",
       "      <th>photon_1lead_eta</th>\n",
       "      <th>photon_1lead_phi</th>\n",
       "      <th>photon_1lead_E</th>\n",
       "      <th>photon_1lead_etcone20</th>\n",
       "      <th>photon_2lead_pt</th>\n",
       "      <th>photon_2lead_eta</th>\n",
       "      <th>photon_2lead_phi</th>\n",
       "      <th>photon_2lead_E</th>\n",
       "      <th>...</th>\n",
       "      <th>lep_phi_mean</th>\n",
       "      <th>lep_phi_std</th>\n",
       "      <th>lep_E_sum</th>\n",
       "      <th>lep_theta_mean</th>\n",
       "      <th>lep_theta_std</th>\n",
       "      <th>lep_charge_sum</th>\n",
       "      <th>lep_ptcone30_sum</th>\n",
       "      <th>lep_etcone20_sum</th>\n",
       "      <th>label</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34762</th>\n",
       "      <td>2</td>\n",
       "      <td>61186.055</td>\n",
       "      <td>0.745909</td>\n",
       "      <td>1.857736</td>\n",
       "      <td>79011.390</td>\n",
       "      <td>-214.42026</td>\n",
       "      <td>58043.250</td>\n",
       "      <td>-0.610472</td>\n",
       "      <td>-2.745476</td>\n",
       "      <td>69199.020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tt</td>\n",
       "      <td>3.535056e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17522</th>\n",
       "      <td>2</td>\n",
       "      <td>57509.270</td>\n",
       "      <td>-0.547249</td>\n",
       "      <td>-3.109003</td>\n",
       "      <td>66337.810</td>\n",
       "      <td>-519.36290</td>\n",
       "      <td>49587.080</td>\n",
       "      <td>0.501637</td>\n",
       "      <td>-0.035371</td>\n",
       "      <td>55958.066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>gg</td>\n",
       "      <td>6.375784e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47589</th>\n",
       "      <td>2</td>\n",
       "      <td>57872.516</td>\n",
       "      <td>-0.576206</td>\n",
       "      <td>-0.812393</td>\n",
       "      <td>67748.520</td>\n",
       "      <td>-733.14470</td>\n",
       "      <td>46447.676</td>\n",
       "      <td>0.728025</td>\n",
       "      <td>2.388419</td>\n",
       "      <td>59310.140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>gg</td>\n",
       "      <td>6.258318e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>2</td>\n",
       "      <td>156658.190</td>\n",
       "      <td>0.683966</td>\n",
       "      <td>-2.048359</td>\n",
       "      <td>194752.220</td>\n",
       "      <td>-242.73482</td>\n",
       "      <td>37639.945</td>\n",
       "      <td>0.693092</td>\n",
       "      <td>2.438658</td>\n",
       "      <td>47048.370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Z</td>\n",
       "      <td>4.440460e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44652</th>\n",
       "      <td>2</td>\n",
       "      <td>84145.766</td>\n",
       "      <td>0.430823</td>\n",
       "      <td>-1.531170</td>\n",
       "      <td>92076.400</td>\n",
       "      <td>-1019.32764</td>\n",
       "      <td>27554.098</td>\n",
       "      <td>-1.093629</td>\n",
       "      <td>1.668323</td>\n",
       "      <td>45740.984</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>gg</td>\n",
       "      <td>6.191731e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27005</th>\n",
       "      <td>2</td>\n",
       "      <td>66853.030</td>\n",
       "      <td>1.625911</td>\n",
       "      <td>0.868543</td>\n",
       "      <td>176484.560</td>\n",
       "      <td>-913.14026</td>\n",
       "      <td>42325.800</td>\n",
       "      <td>0.195025</td>\n",
       "      <td>-1.363790</td>\n",
       "      <td>43133.280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>VBF</td>\n",
       "      <td>1.540992e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3731</th>\n",
       "      <td>2</td>\n",
       "      <td>63004.390</td>\n",
       "      <td>0.887710</td>\n",
       "      <td>-0.333082</td>\n",
       "      <td>89502.695</td>\n",
       "      <td>-1976.89830</td>\n",
       "      <td>57729.113</td>\n",
       "      <td>-0.742256</td>\n",
       "      <td>-1.374505</td>\n",
       "      <td>74375.516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>VBF</td>\n",
       "      <td>5.338199e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19695</th>\n",
       "      <td>2</td>\n",
       "      <td>81314.836</td>\n",
       "      <td>1.139199</td>\n",
       "      <td>-2.058962</td>\n",
       "      <td>140037.980</td>\n",
       "      <td>-794.09393</td>\n",
       "      <td>65658.380</td>\n",
       "      <td>0.297407</td>\n",
       "      <td>-0.524126</td>\n",
       "      <td>68583.625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>VBF</td>\n",
       "      <td>1.139801e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40489</th>\n",
       "      <td>2</td>\n",
       "      <td>73674.766</td>\n",
       "      <td>0.438450</td>\n",
       "      <td>-2.611038</td>\n",
       "      <td>80870.510</td>\n",
       "      <td>-242.98242</td>\n",
       "      <td>53581.746</td>\n",
       "      <td>-0.156901</td>\n",
       "      <td>1.061709</td>\n",
       "      <td>54242.637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>gg</td>\n",
       "      <td>9.122884e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5236</th>\n",
       "      <td>2</td>\n",
       "      <td>75851.930</td>\n",
       "      <td>0.718618</td>\n",
       "      <td>-2.071324</td>\n",
       "      <td>96294.810</td>\n",
       "      <td>-349.21970</td>\n",
       "      <td>39224.625</td>\n",
       "      <td>-1.011326</td>\n",
       "      <td>2.746928</td>\n",
       "      <td>61052.750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tt</td>\n",
       "      <td>5.701518e-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38282 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       photon_n  photon_1lead_pt  photon_1lead_eta  photon_1lead_phi  \\\n",
       "34762         2        61186.055          0.745909          1.857736   \n",
       "17522         2        57509.270         -0.547249         -3.109003   \n",
       "47589         2        57872.516         -0.576206         -0.812393   \n",
       "1178          2       156658.190          0.683966         -2.048359   \n",
       "44652         2        84145.766          0.430823         -1.531170   \n",
       "...         ...              ...               ...               ...   \n",
       "27005         2        66853.030          1.625911          0.868543   \n",
       "3731          2        63004.390          0.887710         -0.333082   \n",
       "19695         2        81314.836          1.139199         -2.058962   \n",
       "40489         2        73674.766          0.438450         -2.611038   \n",
       "5236          2        75851.930          0.718618         -2.071324   \n",
       "\n",
       "       photon_1lead_E  photon_1lead_etcone20  photon_2lead_pt  \\\n",
       "34762       79011.390             -214.42026        58043.250   \n",
       "17522       66337.810             -519.36290        49587.080   \n",
       "47589       67748.520             -733.14470        46447.676   \n",
       "1178       194752.220             -242.73482        37639.945   \n",
       "44652       92076.400            -1019.32764        27554.098   \n",
       "...               ...                    ...              ...   \n",
       "27005      176484.560             -913.14026        42325.800   \n",
       "3731        89502.695            -1976.89830        57729.113   \n",
       "19695      140037.980             -794.09393        65658.380   \n",
       "40489       80870.510             -242.98242        53581.746   \n",
       "5236        96294.810             -349.21970        39224.625   \n",
       "\n",
       "       photon_2lead_eta  photon_2lead_phi  photon_2lead_E  ...  lep_phi_mean  \\\n",
       "34762         -0.610472         -2.745476       69199.020  ...           0.0   \n",
       "17522          0.501637         -0.035371       55958.066  ...           0.0   \n",
       "47589          0.728025          2.388419       59310.140  ...           0.0   \n",
       "1178           0.693092          2.438658       47048.370  ...           0.0   \n",
       "44652         -1.093629          1.668323       45740.984  ...           0.0   \n",
       "...                 ...               ...             ...  ...           ...   \n",
       "27005          0.195025         -1.363790       43133.280  ...           0.0   \n",
       "3731          -0.742256         -1.374505       74375.516  ...           0.0   \n",
       "19695          0.297407         -0.524126       68583.625  ...           0.0   \n",
       "40489         -0.156901          1.061709       54242.637  ...           0.0   \n",
       "5236          -1.011326          2.746928       61052.750  ...           0.0   \n",
       "\n",
       "       lep_phi_std  lep_E_sum  lep_theta_mean  lep_theta_std  lep_charge_sum  \\\n",
       "34762          0.0        0.0             0.0            0.0               0   \n",
       "17522          0.0        0.0             0.0            0.0               0   \n",
       "47589          0.0        0.0             0.0            0.0               0   \n",
       "1178           0.0        0.0             0.0            0.0               0   \n",
       "44652          0.0        0.0             0.0            0.0               0   \n",
       "...            ...        ...             ...            ...             ...   \n",
       "27005          0.0        0.0             0.0            0.0               0   \n",
       "3731           0.0        0.0             0.0            0.0               0   \n",
       "19695          0.0        0.0             0.0            0.0               0   \n",
       "40489          0.0        0.0             0.0            0.0               0   \n",
       "5236           0.0        0.0             0.0            0.0               0   \n",
       "\n",
       "       lep_ptcone30_sum  lep_etcone20_sum  label        weight  \n",
       "34762               0.0               0.0     tt  3.535056e-12  \n",
       "17522               0.0               0.0     gg  6.375784e-08  \n",
       "47589               0.0               0.0     gg  6.258318e-08  \n",
       "1178                0.0               0.0      Z  4.440460e-09  \n",
       "44652               0.0               0.0     gg  6.191731e-08  \n",
       "...                 ...               ...    ...           ...  \n",
       "27005               0.0               0.0    VBF  1.540992e-08  \n",
       "3731                0.0               0.0    VBF  5.338199e-09  \n",
       "19695               0.0               0.0    VBF  1.139801e-08  \n",
       "40489               0.0               0.0     gg  9.122884e-08  \n",
       "5236                0.0               0.0     tt  5.701518e-12  \n",
       "\n",
       "[38282 rows x 25 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BaselineFeaturePipeline = \\\n",
    "    pipeline_maker(Pipeline, steps=[\n",
    "        ('keep_features', KeepFeatures(['photon_n', 'photon_1lead_pt',\n",
    "                                        'photon_1lead_eta', 'photon_1lead_phi',\n",
    "                                        'photon_1lead_E', 'photon_1lead_etcone20',\n",
    "                                        'photon_2lead_pt', 'photon_2lead_eta',\n",
    "                                        'photon_2lead_phi', 'photon_2lead_E',\n",
    "                                        'photon_2lead_etcone20', 'h_mass',\n",
    "                                        'met_et', 'met_phi', 'lep_pt_sum',\n",
    "                                        'lep_phi_mean', 'lep_phi_std', 'lep_E_sum',\n",
    "                                        'lep_theta_mean', 'lep_theta_std',\n",
    "                                        'lep_charge_sum', 'lep_ptcone30_sum',\n",
    "                                        'lep_etcone20_sum', 'label', 'weight'])),\n",
    "        ('filna', FillNa(0.))\n",
    "    ])\n",
    "baseline_feature_pipeline = BaselineFeaturePipeline().fit(d_train)\n",
    "baseline_features = baseline_feature_pipeline.transform(d_train)\n",
    "baseline_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def split_targets(data, targets, drop=None):\n",
    "    flat_target = False\n",
    "    if not isinstance(targets, list):\n",
    "        flat_target = True\n",
    "    to_drop = drop if drop is not None else []\n",
    "    to_drop += targets if not flat_target else [targets]\n",
    "    \n",
    "    Xs = data.drop(columns=to_drop)\n",
    "    ys = data[targets]\n",
    "    \n",
    "    return Xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a baseline model we pick RandomForestClassifier. RandomForest is useful to pick relevant features when the dataset possibly includes correlated features. Correlations are weakend because RandomForest picks a subset of features to train each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = RandomForestClassifier(random_state=RANDOM_STATE).fit(*split_targets(baseline_features, \"label\", drop=[\"weight\"]), sample_weight=baseline_features[\"weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def extract_importances(feature_importances, feature_names):\n",
    "    return pd.DataFrame(zip(feature_names, feature_importances),\n",
    "                                      columns=[\"feature\", \"score\"]) \\\n",
    "             .set_index(\"feature\") \\\n",
    "             .sort_values(\"score\", axis=\"index\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def get_ensemble_feature_importances(ensemble_model):\n",
    "    importances = []\n",
    "    for est in ensemble_model.estimators_:\n",
    "        importances.append(est.steps[-1][1].feature_importances_)\n",
    "    importances = np.vstack(importances)\n",
    "    return np.median(importances, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def report_importances(model, X_test):\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        feature_importances = model.feature_importances_\n",
    "    elif hasattr(model, \"estimators_\") and hasattr(model.estimators_[0].steps[-1][1], \"feature_importances_\"):\n",
    "        feature_importances = get_ensemble_feature_importances(model)\n",
    "    else:\n",
    "        print(\"Model doesn't seem to provide feature importances. Skip feature importances report\")\n",
    "        return\n",
    "    nice_importances = extract_importances(feature_importances, X_test.columns)\n",
    "    print(\"Feature importances\")\n",
    "    display(nice_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def report_class_summary(model, feature_pipeline, X_test, y_test, weights=None):\n",
    "    if not hasattr(model, \"classes_\"):\n",
    "        print(\"Model doesn't seem to be a classificator. Skip classification summary report\")\n",
    "        return\n",
    "    classes = model.classes_\n",
    "    labeler = feature_pipeline.named_steps.get(\"labels\")\n",
    "    if labeler is not None:\n",
    "        classes = labeler.transformer.inverse_transform(classes)\n",
    "    print(\"Model classes\")\n",
    "    display(classes)\n",
    "    pred = model.predict(X_test)\n",
    "    conf = confusion_matrix(y_test, pred)\n",
    "    print(\"Confusion matrix, normalized\")\n",
    "    display(conf)\n",
    "    conf = confusion_matrix(y_test, pred, sample_weight=weights)\n",
    "    print(\"-Log confusion matrix of weights\")\n",
    "    display(-np.log(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def report_descriptive(X_test, y_test, weights=None):\n",
    "    data = pd.concat([X_test, y_test], axis=\"columns\")\n",
    "    if weights is not None:\n",
    "        data = data.assign(weight=weights)\n",
    "        \n",
    "    if \"weight\" in X_test.columns:\n",
    "        weight_sums = data.groupby(\"label\")[\"weight\"].sum()\n",
    "        print(\"Sum of weights\")\n",
    "        display(weight_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def report(model, feature_pipeline, d_test, drop=None):\n",
    "    X_test, y_test = split_targets(d_test, \"label\", drop=drop)\n",
    "    weights = d_test[\"weight\"]\n",
    "    report_descriptive(X_test, y_test, weights=weights)\n",
    "    report_class_summary(model, feature_pipeline, X_test, y_test, weights=weights)\n",
    "    report_importances(model, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has been trained, let's see the report describing the baseline.\n",
    "\n",
    "We have 5 classes and a confusion matrix for them.\n",
    "\n",
    "We also provide confusion matrix for weights in somewhat sophisticated format. Due to weights can differ by orders of magnitude, it is easier to compare logarithms. Since all weights are < 1, it is convenient to remove minus sign close to numbers in the matrix. As a result, the smaller number on the diagonal of the matrix the better.\n",
    "\n",
    "Finally we show feature importances coming from decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model classes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['VBF', 'Wp', 'Z', 'gg', 'tt'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, normalized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 694,    1,    8, 1375,   23],\n",
       "       [ 112,   34,    6,  209,   77],\n",
       "       [ 268,    3,   62,  543,   60],\n",
       "       [ 659,    4,   14, 4455,   22],\n",
       "       [ 423,   60,   68,  718,  737]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Log confusion matrix of weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[11.96644646, 19.01619873, 16.59460066, 11.26966949, 15.45725517],\n",
       "       [13.88850884, 15.11556093, 16.74824984, 13.31878079, 14.35456198],\n",
       "       [13.7923222 , 18.15891606, 15.38583939, 13.10985937, 15.40550347],\n",
       "       [10.29663549, 15.61085439, 14.29582299,  8.36584919, 13.86901952],\n",
       "       [20.4390199 , 22.33854399, 22.1223988 , 20.04203331, 19.98587471]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>photon_1lead_pt</th>\n",
       "      <td>0.091186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_2lead_pt</th>\n",
       "      <td>0.086963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>met_et</th>\n",
       "      <td>0.076756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_1lead_E</th>\n",
       "      <td>0.076532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_2lead_etcone20</th>\n",
       "      <td>0.074936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_2lead_eta</th>\n",
       "      <td>0.072835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_2lead_E</th>\n",
       "      <td>0.071216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_1lead_eta</th>\n",
       "      <td>0.069814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_1lead_phi</th>\n",
       "      <td>0.069288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_2lead_phi</th>\n",
       "      <td>0.069123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_1lead_etcone20</th>\n",
       "      <td>0.069021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>met_phi</th>\n",
       "      <td>0.068597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h_mass</th>\n",
       "      <td>0.068548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lep_pt_sum</th>\n",
       "      <td>0.009934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lep_E_sum</th>\n",
       "      <td>0.007029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lep_charge_sum</th>\n",
       "      <td>0.004594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lep_theta_std</th>\n",
       "      <td>0.002493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lep_phi_std</th>\n",
       "      <td>0.002314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_n</th>\n",
       "      <td>0.002274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lep_theta_mean</th>\n",
       "      <td>0.002241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lep_etcone20_sum</th>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lep_phi_mean</th>\n",
       "      <td>0.001767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lep_ptcone30_sum</th>\n",
       "      <td>0.000641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          score\n",
       "feature                        \n",
       "photon_1lead_pt        0.091186\n",
       "photon_2lead_pt        0.086963\n",
       "met_et                 0.076756\n",
       "photon_1lead_E         0.076532\n",
       "photon_2lead_etcone20  0.074936\n",
       "photon_2lead_eta       0.072835\n",
       "photon_2lead_E         0.071216\n",
       "photon_1lead_eta       0.069814\n",
       "photon_1lead_phi       0.069288\n",
       "photon_2lead_phi       0.069123\n",
       "photon_1lead_etcone20  0.069021\n",
       "met_phi                0.068597\n",
       "h_mass                 0.068548\n",
       "lep_pt_sum             0.009934\n",
       "lep_E_sum              0.007029\n",
       "lep_charge_sum         0.004594\n",
       "lep_theta_std          0.002493\n",
       "lep_phi_std            0.002314\n",
       "photon_n               0.002274\n",
       "lep_theta_mean         0.002241\n",
       "lep_etcone20_sum       0.001900\n",
       "lep_phi_mean           0.001767\n",
       "lep_ptcone30_sum       0.000641"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "report(baseline_model, baseline_feature_pipeline, baseline_feature_pipeline.transform(d_test), drop=[\"weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently `gg` events are so many, that other events become classified as `gg` just statistically. This means that the dataset is imbalanced, and we should find some approach to target this issue.\n",
    "\n",
    "We also choose subset of features to work with. We pick only significantly relevant features. See the list of them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def mask_by_levels(data, threshold=None, limit=None):\n",
    "    thr_mask = np.array(True)\n",
    "    if threshold is not None:\n",
    "        if isinstance(threshold, dict):\n",
    "            thr_mask = np.all([data[k] > v for k,v in threshold.items()])\n",
    "        else:\n",
    "            thr_mask = np.all(data > threshold, axis=1)\n",
    "    \n",
    "    lim_mask = np.array(True)\n",
    "    if limit is not None:\n",
    "        if isinstance(limit, dict):\n",
    "            lim_mask = np.all([data[k] < v for k,v in limit.items()])\n",
    "        else:\n",
    "            lim_mask = np.all(data < limit, axis=1)\n",
    "            \n",
    "    mask = thr_mask & lim_mask\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def filter_by_levels(data, threshold=None, limit=None):\n",
    "    mask = mask_by_levels(data, threshold=threshold, limit=limit)\n",
    "    \n",
    "    if np.all(mask):\n",
    "        return data.copy()\n",
    "    else:\n",
    "        return data[mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['photon_1lead_pt',\n",
       " 'photon_2lead_pt',\n",
       " 'met_et',\n",
       " 'photon_1lead_E',\n",
       " 'photon_2lead_etcone20',\n",
       " 'photon_2lead_eta',\n",
       " 'photon_2lead_E',\n",
       " 'photon_1lead_eta',\n",
       " 'photon_1lead_phi',\n",
       " 'photon_2lead_phi',\n",
       " 'photon_1lead_etcone20',\n",
       " 'met_phi',\n",
       " 'h_mass']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subcolumns = list(filter_by_levels(\n",
    "                extract_importances(\n",
    "                    baseline_model.feature_importances_,\n",
    "                    split_targets(baseline_features, \"label\", drop=[\"weight\"])[0].columns\n",
    "                ),\n",
    "                threshold=0.05\n",
    "             ).index)\n",
    "subcolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "class WithColumns(TransformerMixin):\n",
    "    def __init__(self, transformer, columns=None):\n",
    "        self.columns = columns\n",
    "        self.transformer = transformer\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.transformer.fit(X[self.columns])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        res = X.copy()\n",
    "        res[self.columns] = self.transformer.transform(res[self.columns])\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "class ReplaceValues(TransformerMixin):\n",
    "    def __init__(self, mapping):\n",
    "        self.mapping = mapping\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        res = X.replace(self.mapping)\n",
    "        return res\n",
    "# WithColumns(ReplaceValues({\n",
    "#                                 \"VBF\": \"other\",\n",
    "#                                 \"Wp\": \"other\",\n",
    "#                                 \"Z\": \"other\",\n",
    "#                                 \"gg\": \"other\"\n",
    "#                                }), columns=\"label\").fit_transform(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "class DropByLevel(TransformerMixin):\n",
    "    def __init__(self, column, threshold=None, limit=None):\n",
    "        self.column = column\n",
    "        self.threshold = threshold\n",
    "        self.limit = limit\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        mask = mask_by_levels(X[self.column], threshold=self.threshold, limit=self.limit)\n",
    "        \n",
    "        if np.all(mask):\n",
    "            return X.copy()\n",
    "        \n",
    "        return X[mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "class DropByValues(TransformerMixin):\n",
    "    def __init__(self, column, values):\n",
    "        self.column = column\n",
    "        self.values = values\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        mask = ~np.any([X[self.column] == v for v in self.values], axis=0)\n",
    "        \n",
    "        if np.all(mask):\n",
    "            return X.copy()\n",
    "        \n",
    "        return X[mask].copy()\n",
    "# DropByValues(\"label\", [\"tt\"]).fit_transform(d_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature processing for next to baseline model, we name it `significant imbalanced model` is a bit more complex. As before, we fill empty values for chosen features with zeros.\n",
    "\n",
    "We also drop `tt`, `Wp` and `Z` events for this step. We try to focus on events that have similar weight per event. `Wp` and `Z` are few but they are very heavy, while `tt` are quite a lot but weight is tiny. It is quite hard to handle such a mixture together.\n",
    "\n",
    "Now we end up with `gg` and `VBF` events. The dataset is still imbalanced from the number of events perspective, but at the same time we can bother less about their weights.\n",
    "\n",
    "As a part of feature processing, we also drop negative weights. That's just a technical issue, because the algorithm we are going to apply can't handle negative weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>photon_1lead_pt</th>\n",
       "      <th>photon_2lead_pt</th>\n",
       "      <th>met_et</th>\n",
       "      <th>photon_1lead_E</th>\n",
       "      <th>photon_2lead_etcone20</th>\n",
       "      <th>photon_2lead_eta</th>\n",
       "      <th>photon_2lead_E</th>\n",
       "      <th>photon_1lead_eta</th>\n",
       "      <th>photon_1lead_phi</th>\n",
       "      <th>photon_2lead_phi</th>\n",
       "      <th>photon_1lead_etcone20</th>\n",
       "      <th>met_phi</th>\n",
       "      <th>h_mass</th>\n",
       "      <th>label</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17522</th>\n",
       "      <td>57509.270</td>\n",
       "      <td>49587.080</td>\n",
       "      <td>24200.309</td>\n",
       "      <td>66337.810</td>\n",
       "      <td>-94.554214</td>\n",
       "      <td>0.501637</td>\n",
       "      <td>55958.066</td>\n",
       "      <td>-0.547249</td>\n",
       "      <td>-3.109003</td>\n",
       "      <td>-0.035371</td>\n",
       "      <td>-519.36290</td>\n",
       "      <td>-0.051363</td>\n",
       "      <td>119516.130</td>\n",
       "      <td>1</td>\n",
       "      <td>6.375784e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47589</th>\n",
       "      <td>57872.516</td>\n",
       "      <td>46447.676</td>\n",
       "      <td>40232.438</td>\n",
       "      <td>67748.520</td>\n",
       "      <td>-1234.568200</td>\n",
       "      <td>0.728025</td>\n",
       "      <td>59310.140</td>\n",
       "      <td>-0.576206</td>\n",
       "      <td>-0.812393</td>\n",
       "      <td>2.388419</td>\n",
       "      <td>-733.14470</td>\n",
       "      <td>2.524073</td>\n",
       "      <td>119186.875</td>\n",
       "      <td>1</td>\n",
       "      <td>6.258318e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44652</th>\n",
       "      <td>84145.766</td>\n",
       "      <td>27554.098</td>\n",
       "      <td>51859.027</td>\n",
       "      <td>92076.400</td>\n",
       "      <td>-86.023605</td>\n",
       "      <td>-1.093629</td>\n",
       "      <td>45740.984</td>\n",
       "      <td>0.430823</td>\n",
       "      <td>-1.531170</td>\n",
       "      <td>1.668323</td>\n",
       "      <td>-1019.32764</td>\n",
       "      <td>1.554219</td>\n",
       "      <td>129246.170</td>\n",
       "      <td>1</td>\n",
       "      <td>6.191731e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29709</th>\n",
       "      <td>73832.836</td>\n",
       "      <td>41363.820</td>\n",
       "      <td>54375.840</td>\n",
       "      <td>81773.960</td>\n",
       "      <td>-1200.018900</td>\n",
       "      <td>-0.561445</td>\n",
       "      <td>48056.250</td>\n",
       "      <td>0.459741</td>\n",
       "      <td>-2.871648</td>\n",
       "      <td>0.420344</td>\n",
       "      <td>-786.47240</td>\n",
       "      <td>0.346971</td>\n",
       "      <td>123697.970</td>\n",
       "      <td>1</td>\n",
       "      <td>4.847792e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43025</th>\n",
       "      <td>63447.668</td>\n",
       "      <td>51555.555</td>\n",
       "      <td>45032.043</td>\n",
       "      <td>108673.080</td>\n",
       "      <td>-328.382870</td>\n",
       "      <td>-0.104425</td>\n",
       "      <td>51836.906</td>\n",
       "      <td>1.132488</td>\n",
       "      <td>-0.659084</td>\n",
       "      <td>1.424528</td>\n",
       "      <td>-1129.62730</td>\n",
       "      <td>2.748239</td>\n",
       "      <td>119599.125</td>\n",
       "      <td>1</td>\n",
       "      <td>3.888226e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35735</th>\n",
       "      <td>48830.830</td>\n",
       "      <td>48478.434</td>\n",
       "      <td>10524.696</td>\n",
       "      <td>61091.420</td>\n",
       "      <td>-323.829160</td>\n",
       "      <td>-0.914525</td>\n",
       "      <td>70203.940</td>\n",
       "      <td>0.694590</td>\n",
       "      <td>2.793305</td>\n",
       "      <td>-1.118841</td>\n",
       "      <td>-579.96560</td>\n",
       "      <td>-0.686762</td>\n",
       "      <td>122219.350</td>\n",
       "      <td>1</td>\n",
       "      <td>6.394328e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27005</th>\n",
       "      <td>66853.030</td>\n",
       "      <td>42325.800</td>\n",
       "      <td>71642.560</td>\n",
       "      <td>176484.560</td>\n",
       "      <td>-1256.716200</td>\n",
       "      <td>0.195025</td>\n",
       "      <td>43133.280</td>\n",
       "      <td>1.625911</td>\n",
       "      <td>0.868543</td>\n",
       "      <td>-1.363790</td>\n",
       "      <td>-913.14026</td>\n",
       "      <td>-1.225946</td>\n",
       "      <td>127275.800</td>\n",
       "      <td>0</td>\n",
       "      <td>1.540992e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3731</th>\n",
       "      <td>63004.390</td>\n",
       "      <td>57729.113</td>\n",
       "      <td>78592.805</td>\n",
       "      <td>89502.695</td>\n",
       "      <td>-372.513120</td>\n",
       "      <td>-0.742256</td>\n",
       "      <td>74375.516</td>\n",
       "      <td>0.887710</td>\n",
       "      <td>-0.333082</td>\n",
       "      <td>-1.374505</td>\n",
       "      <td>-1976.89830</td>\n",
       "      <td>2.781973</td>\n",
       "      <td>125533.550</td>\n",
       "      <td>0</td>\n",
       "      <td>5.338199e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19695</th>\n",
       "      <td>81314.836</td>\n",
       "      <td>65658.380</td>\n",
       "      <td>59611.215</td>\n",
       "      <td>140037.980</td>\n",
       "      <td>-868.619570</td>\n",
       "      <td>0.297407</td>\n",
       "      <td>68583.625</td>\n",
       "      <td>1.139199</td>\n",
       "      <td>-2.058962</td>\n",
       "      <td>-0.524126</td>\n",
       "      <td>-794.09393</td>\n",
       "      <td>1.200092</td>\n",
       "      <td>122452.530</td>\n",
       "      <td>0</td>\n",
       "      <td>1.139801e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40489</th>\n",
       "      <td>73674.766</td>\n",
       "      <td>53581.746</td>\n",
       "      <td>27399.121</td>\n",
       "      <td>80870.510</td>\n",
       "      <td>-646.382900</td>\n",
       "      <td>-0.156901</td>\n",
       "      <td>54242.637</td>\n",
       "      <td>0.438450</td>\n",
       "      <td>-2.611038</td>\n",
       "      <td>1.061709</td>\n",
       "      <td>-242.98242</td>\n",
       "      <td>-0.364433</td>\n",
       "      <td>128337.260</td>\n",
       "      <td>1</td>\n",
       "      <td>9.122884e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25855 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       photon_1lead_pt  photon_2lead_pt     met_et  photon_1lead_E  \\\n",
       "17522        57509.270        49587.080  24200.309       66337.810   \n",
       "47589        57872.516        46447.676  40232.438       67748.520   \n",
       "44652        84145.766        27554.098  51859.027       92076.400   \n",
       "29709        73832.836        41363.820  54375.840       81773.960   \n",
       "43025        63447.668        51555.555  45032.043      108673.080   \n",
       "...                ...              ...        ...             ...   \n",
       "35735        48830.830        48478.434  10524.696       61091.420   \n",
       "27005        66853.030        42325.800  71642.560      176484.560   \n",
       "3731         63004.390        57729.113  78592.805       89502.695   \n",
       "19695        81314.836        65658.380  59611.215      140037.980   \n",
       "40489        73674.766        53581.746  27399.121       80870.510   \n",
       "\n",
       "       photon_2lead_etcone20  photon_2lead_eta  photon_2lead_E  \\\n",
       "17522             -94.554214          0.501637       55958.066   \n",
       "47589           -1234.568200          0.728025       59310.140   \n",
       "44652             -86.023605         -1.093629       45740.984   \n",
       "29709           -1200.018900         -0.561445       48056.250   \n",
       "43025            -328.382870         -0.104425       51836.906   \n",
       "...                      ...               ...             ...   \n",
       "35735            -323.829160         -0.914525       70203.940   \n",
       "27005           -1256.716200          0.195025       43133.280   \n",
       "3731             -372.513120         -0.742256       74375.516   \n",
       "19695            -868.619570          0.297407       68583.625   \n",
       "40489            -646.382900         -0.156901       54242.637   \n",
       "\n",
       "       photon_1lead_eta  photon_1lead_phi  photon_2lead_phi  \\\n",
       "17522         -0.547249         -3.109003         -0.035371   \n",
       "47589         -0.576206         -0.812393          2.388419   \n",
       "44652          0.430823         -1.531170          1.668323   \n",
       "29709          0.459741         -2.871648          0.420344   \n",
       "43025          1.132488         -0.659084          1.424528   \n",
       "...                 ...               ...               ...   \n",
       "35735          0.694590          2.793305         -1.118841   \n",
       "27005          1.625911          0.868543         -1.363790   \n",
       "3731           0.887710         -0.333082         -1.374505   \n",
       "19695          1.139199         -2.058962         -0.524126   \n",
       "40489          0.438450         -2.611038          1.061709   \n",
       "\n",
       "       photon_1lead_etcone20   met_phi      h_mass  label        weight  \n",
       "17522             -519.36290 -0.051363  119516.130      1  6.375784e-08  \n",
       "47589             -733.14470  2.524073  119186.875      1  6.258318e-08  \n",
       "44652            -1019.32764  1.554219  129246.170      1  6.191731e-08  \n",
       "29709             -786.47240  0.346971  123697.970      1  4.847792e-08  \n",
       "43025            -1129.62730  2.748239  119599.125      1  3.888226e-08  \n",
       "...                      ...       ...         ...    ...           ...  \n",
       "35735             -579.96560 -0.686762  122219.350      1  6.394328e-08  \n",
       "27005             -913.14026 -1.225946  127275.800      0  1.540992e-08  \n",
       "3731             -1976.89830  2.781973  125533.550      0  5.338199e-09  \n",
       "19695             -794.09393  1.200092  122452.530      0  1.139801e-08  \n",
       "40489             -242.98242 -0.364433  128337.260      1  9.122884e-08  \n",
       "\n",
       "[25855 rows x 15 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SignificantFeaturePipeline = \\\n",
    "    pipeline_maker(Pipeline, steps=[\n",
    "        ('keep_features', KeepFeatures(subcolumns + [\"label\", \"weight\"])),\n",
    "        ('filna', FillNa(0.)),\n",
    "        ('drop_tt', DropByValues(\"label\", [\"tt\", \"Wp\", \"Z\"])),\n",
    "        ('drop_neg_weight', DropByLevel([\"weight\"], threshold=0)),\n",
    "        ('labels', WithColumns(LabelEncoder(), columns=\"label\"))\n",
    "    ])\n",
    "significant_feature_pipeline = SignificantFeaturePipeline().fit(d_train)\n",
    "significant_features = significant_feature_pipeline.transform(d_train)\n",
    "significant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(significant_features[\"weight\"] > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the reference about usage of [bagging for imbalanced datasets](https://machinelearningmastery.com/bagging-and-random-forest-for-imbalanced-classification/) we focus on the model of Easy Ensemble. It is an ensemble model, where AdaBoost (a variant of BDT) is used as a base estimator. The goal is to downsample major class in order to balance the trainig sample.\n",
    "\n",
    "To avoid throwing out the data we train 6 different AdaBoosts on different samples, where minority class is always entirely included, while events from majority class are downsampled to fit number of events in the minority class.\n",
    "\n",
    "After 6 different base estimators have been trained, the voting procedure decides on the prediction for the specific test event.\n",
    "\n",
    "\n",
    "\\* Note: we had to monkey-patch the AdaBoostClassifier to allow EasyEnsemble implementation to use sample weights. See details in the full version of the notebook in the github repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "class WeightedAdaBoost(AdaBoostClassifier):\n",
    "    def __init__(self,\n",
    "                 base_estimator=None, *,\n",
    "                 n_estimators=50,\n",
    "                 learning_rate=1.,\n",
    "                 algorithm='SAMME.R',\n",
    "                 random_state=None, weights_column=None):\n",
    "\n",
    "        super().__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            random_state=random_state)\n",
    "        self.weights_column = weights_column\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        super().fit(np.delete(X, self.weights_column, 1), y, sample_weight=X[:, self.weights_column])\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return super().predict(np.delete(X, self.weights_column, 1))\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return super().predict_proba(np.delete(X, self.weights_column, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_imbal_model = EasyEnsembleClassifier( \\\n",
    "                                          n_estimators=6,\n",
    "                                          base_estimator=WeightedAdaBoost(weights_column=list(split_targets(significant_features, \"label\")[0].columns).index(\"weight\")),\n",
    "                                          random_state=RANDOM_STATE,\n",
    "                                          sampling_strategy=\"not minority\",\n",
    "                                          replacement=False\n",
    "                                         ) \\\n",
    "                   .fit(*split_targets(significant_features, \"label\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a report following the same structure as we had for the baseline model.\n",
    "\n",
    "Here we have 2 classes with a simple confusion matrix. It shows that we still didn't manage to fight the imbalance.\n",
    "\n",
    "Since here we have ensemble of BDTs, as feature importance we show median of scores per feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    0.000019\n",
       "1    0.000268\n",
       "Name: weight, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model classes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['VBF', 'gg'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, normalized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  81, 1969],\n",
       "       [  57, 4985]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Log confusion matrix of weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[14.0949592 , 10.88880119],\n",
       "       [12.69525536,  8.23451207]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>photon_1lead_pt</th>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_2lead_eta</th>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_1lead_eta</th>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_2lead_pt</th>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_2lead_E</th>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>met_et</th>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_1lead_E</th>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_2lead_etcone20</th>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_2lead_phi</th>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>met_phi</th>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h_mass</th>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_1lead_etcone20</th>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_1lead_phi</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       score\n",
       "feature                     \n",
       "photon_1lead_pt         0.22\n",
       "photon_2lead_eta        0.15\n",
       "photon_1lead_eta        0.12\n",
       "photon_2lead_pt         0.08\n",
       "photon_2lead_E          0.08\n",
       "met_et                  0.07\n",
       "photon_1lead_E          0.05\n",
       "photon_2lead_etcone20   0.05\n",
       "photon_2lead_phi        0.04\n",
       "met_phi                 0.04\n",
       "h_mass                  0.04\n",
       "photon_1lead_etcone20   0.02\n",
       "photon_1lead_phi        0.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "report(sign_imbal_model, significant_feature_pipeline, significant_feature_pipeline.transform(d_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost can be treated as a specific representative of the class of the additive models. XGBoost is more generic and allows optimizing any model from the class. XGBoost uses gradient descent approach to optimize parameters of the model, while AdaBoost applies reweighting to the datasample at each step, in order to train on the most outlying data points. The latter effectively leads to exponential loss function.\n",
    "\n",
    "More on AdaBoost vs XGBoost comparison:\n",
    "\n",
    "* https://datascience.stackexchange.com/questions/39193/adaboost-vs-gradient-boosting\n",
    "* https://stats.stackexchange.com/questions/164233/intuitive-explanations-of-differences-between-gradient-boosting-trees-gbm-ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_cv = GridSearchCV(XGBClassifier(random_state=RANDOM_STATE,\n",
    "                                        objective=\"multi:softmax\",\n",
    "                                        num_class=2),\n",
    "                          param_grid={\n",
    "                            \"learning_rate\": [1E-10, 1E-6],\n",
    "                            \"max_depth\": [10, 20] ,\n",
    "                            \"gamma\": [0.1, 0.5, 1.2],\n",
    "                            \"reg_alpha\": [1E-3, 1E-1],\n",
    "                            \"reg_lambda\": [1E-3, 1E-1]\n",
    "                          }, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estim...\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None, validate_parameters=None,\n",
       "                                     verbosity=None),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'gamma': [0.1, 0.5, 1.2],\n",
       "                         'learning_rate': [1e-10, 1e-06], 'max_depth': [10, 20],\n",
       "                         'reg_alpha': [0.001, 0.1],\n",
       "                         'reg_lambda': [0.001, 0.1]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_cv.fit(*split_targets(significant_features, \"label\", drop=[\"weight\"]),\n",
    "               sample_weight=significant_features[\"weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0.1, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=1e-10, max_delta_step=0, max_depth=10,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_class=2, num_parallel_tree=1,\n",
       "              objective='multi:softmax', random_state=23, reg_alpha=0.001,\n",
       "              reg_lambda=0.001, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model classes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['VBF', 'gg'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, normalized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2050,    0],\n",
       "       [5042,    0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Log confusion matrix of weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/cern.ch/user/a/ananiev/projects/venvs/sandbox/lib64/python3.6/site-packages/ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[10.84908833,         inf],\n",
       "       [ 8.22302453,         inf]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/cern.ch/user/a/ananiev/projects/venvs/sandbox/lib64/python3.6/site-packages/xgboost/sklearn.py:691: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return all_features / all_features.sum()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>photon_1lead_pt</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_2lead_pt</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>met_et</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_1lead_E</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_2lead_etcone20</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_2lead_eta</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_2lead_E</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_1lead_eta</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_1lead_phi</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_2lead_phi</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photon_1lead_etcone20</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>met_phi</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h_mass</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       score\n",
       "feature                     \n",
       "photon_1lead_pt          NaN\n",
       "photon_2lead_pt          NaN\n",
       "met_et                   NaN\n",
       "photon_1lead_E           NaN\n",
       "photon_2lead_etcone20    NaN\n",
       "photon_2lead_eta         NaN\n",
       "photon_2lead_E           NaN\n",
       "photon_1lead_eta         NaN\n",
       "photon_1lead_phi         NaN\n",
       "photon_2lead_phi         NaN\n",
       "photon_1lead_etcone20    NaN\n",
       "met_phi                  NaN\n",
       "h_mass                   NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "report(xgboost_cv.best_estimator_, significant_feature_pipeline, significant_feature_pipeline.transform(d_test), drop=[\"weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Within this toy project we managed to reproduce Higgs mass peak, while discovering that ATLAS Open Data is not enough to reproduce backgrounds taking part in the process. Moreover, we managed to set up the end-to-end pipeline for feature processing and learning. It is implemented in Python but, thanks to NumPy and \"family\", still not less efficient than C++ codes.\n",
    "\n",
    "Feature processing pipeline, now provides basic set of simple features and can easily be extended to producing more sophisticated collections without loss in speed. We learned that jagged data is the essential feature of in HEP analyses, and future attempts to train on particle data should account on this. Having this in mind, we, however, decided to follow the simplest approach for the sake of building a wider picture of the data analysis pipeline in ATLAS.\n",
    "\n",
    "Finally, several models have been tested in order to classify Higgs boson production mechanisms. All of them show results highly biased towards gluon-gluon fusion. This most probably caused by the fact that the data is imbalanced by both parameters: number of events and event weights. Sometimes minority by number of events can consist majority in weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
