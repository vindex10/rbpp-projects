{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply cuts and export filtered events\n",
    "-------------------------------------------------------\n",
    "\n",
    "Di-photon event selection based on https://arxiv.org/abs/1802.04146"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import chain\n",
    "from multiprocessing.dummy import Process\n",
    "from multiprocessing.dummy import Lock\n",
    "from multiprocessing.dummy import Value\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import uproot\n",
    "\n",
    "from XRootD import client\n",
    "from XRootD.client.flags import OpenFlags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"eosuser.cern.ch//eos/user/a/ananiev/data/\"\n",
    "output_path = \"/afs/cern.ch/user/a/ananiev/cernbox/output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"GamGam\": {\n",
    "        \"MC\": [\n",
    "            (\"mc_341081.ttH125_gamgam.GamGam\", {\"tag\": \"tt\"}),\n",
    "            (\"mc_343981.ggH125_gamgam.GamGam\", {\"tag\": \"gg\"}),\n",
    "            (\"mc_345041.VBFH125_gamgam.GamGam\", {\"tag\": \"VBF\"}),\n",
    "            (\"mc_345318.WpH125J_Wincl_gamgam.GamGam\", {\"tag\": \"Wp\"}),\n",
    "            (\"mc_345319.ZH125J_Zincl_gamgam.GamGam\", {\"tag\": \"Z\"})\n",
    "\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mc = uproot.open(os.path.join(\"root://\", data_path, \"GamGam\", \"MC\", f\"{datasets['GamGam']['MC'][0][0]}.root\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mc_events = next(test_mc[\"mini\"].iterate([\"*\"], entrystop=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_files_with_meta(datasets):\n",
    "    for process_name, process in datasets.items():\n",
    "        print(\"Process: \", process_name)\n",
    "        for type_name, thetype in process.items():\n",
    "            print(\"Type: \", type_name)\n",
    "            for filedata in thetype:\n",
    "                try:\n",
    "                    filename, meta = filedata\n",
    "                except ValueError:\n",
    "                    filename = filedata\n",
    "                    meta = {}\n",
    "                print(\"File: \", filename)\n",
    "                fullpath = os.path.join(\"root://\", data_path, process_name, type_name, f\"{filename}.root\")\n",
    "                yield (process_name, type_name, filename), meta, fullpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_apply_mask(d, mask, fields=None):\n",
    "    if fields is None:\n",
    "        fields = d.keys()\n",
    "    for f in fields:\n",
    "        d[f] = d[f][mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eta2tg_theta(eta):\n",
    "    tg_theta = np.exp(-eta)\n",
    "    tg_theta = 2*tg_theta**2/(1 - tg_theta**2)\n",
    "    return tg_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atlas_two_cosine(events, p1, p2):\n",
    "    tg_theta_1 = eta2tg_theta(events[p1+b\"_eta\"])\n",
    "    tg_theta_2 = eta2tg_theta(events[p2+b\"_eta\"])\n",
    "    cos_delta_phi = np.cos(events[p2+b\"_phi\"] - events[p1+b\"_phi\"])\n",
    "    return (cos_delta_phi + tg_theta_1*tg_theta_2)/np.sqrt((tg_theta_1**2 + 1)*(tg_theta_2**2 + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_photons(events, mask):\n",
    "    macro_mask = mask.copy()\n",
    "    \n",
    "    macro_events = {}\n",
    "    micro_events = {}\n",
    "    \n",
    "    macro_events[b\"photon_n\"] = events[b\"photon_n\"][macro_mask]\n",
    "    macro_events[b\"trigP\"] = events[b\"trigP\"][macro_mask]\n",
    "    n_threshold = macro_events[b\"photon_n\"] >= 2\n",
    "    is_diphoton = macro_events[b\"trigP\"]\n",
    "    macro_mask[macro_mask] = n_threshold & is_diphoton\n",
    "    \n",
    "    micro_events[b\"photon_pt\"] = events[b\"photon_pt\"][macro_mask]\n",
    "    micro_events[b\"photon_eta\"] = events[b\"photon_eta\"][macro_mask]\n",
    "    micro_events[b\"photon_phi\"] = events[b\"photon_phi\"][macro_mask]\n",
    "    micro_events[b\"photon_E\"] = events[b\"photon_E\"][macro_mask]\n",
    "    micro_events[b\"photon_isTightID\"] = events[b\"photon_isTightID\"][macro_mask]\n",
    "    micro_events[b\"photon_trigMatched\"] = events[b\"photon_trigMatched\"][macro_mask]\n",
    "    micro_events[b\"photon_ptcone30\"] = events[b\"photon_ptcone30\"][macro_mask]\n",
    "    micro_events[b\"photon_etcone20\"] = events[b\"photon_etcone20\"][macro_mask]\n",
    "    \n",
    "    \n",
    "    pts = micro_events[b\"photon_pt\"].argsort(ascending=False)\n",
    "    row_indices = np.arange(pts.shape[0])\n",
    "    lead_pts = pts[:, 0]\n",
    "    sublead_pts = pts[:, 1]\n",
    "    \n",
    "    \n",
    "    macro_events[b\"photon_n\"] = macro_events[b\"photon_n\"][macro_mask]\n",
    "    macro_events[b\"photon_1lead_pt\"] = micro_events[b\"photon_pt\"][row_indices, lead_pts]\n",
    "    macro_events[b\"photon_1lead_eta\"] = micro_events[b\"photon_eta\"][row_indices, lead_pts]\n",
    "    macro_events[b\"photon_1lead_phi\"] = micro_events[b\"photon_phi\"][row_indices, lead_pts]\n",
    "    macro_events[b\"photon_1lead_E\"] = micro_events[b\"photon_E\"][row_indices, lead_pts]\n",
    "    macro_events[b\"photon_1lead_isTightID\"] = micro_events[b\"photon_isTightID\"][row_indices, lead_pts]\n",
    "    macro_events[b\"photon_1lead_trigMatched\"] = micro_events[b\"photon_trigMatched\"][row_indices, lead_pts]\n",
    "    macro_events[b\"photon_1lead_ptcone30\"] = micro_events[b\"photon_ptcone30\"][row_indices, lead_pts]\n",
    "    macro_events[b\"photon_1lead_etcone20\"] = micro_events[b\"photon_etcone20\"][row_indices, lead_pts]\n",
    "    macro_events[b\"photon_2lead_pt\"] = micro_events[b\"photon_pt\"][row_indices, sublead_pts]\n",
    "    macro_events[b\"photon_2lead_eta\"] = micro_events[b\"photon_eta\"][row_indices, sublead_pts]\n",
    "    macro_events[b\"photon_2lead_phi\"] = micro_events[b\"photon_phi\"][row_indices, sublead_pts]\n",
    "    macro_events[b\"photon_2lead_E\"] = micro_events[b\"photon_E\"][row_indices, sublead_pts]\n",
    "    macro_events[b\"photon_2lead_isTightID\"] = micro_events[b\"photon_isTightID\"][row_indices, sublead_pts]\n",
    "    macro_events[b\"photon_2lead_trigMatched\"] = micro_events[b\"photon_trigMatched\"][row_indices, sublead_pts]\n",
    "    macro_events[b\"photon_2lead_ptcone30\"] = micro_events[b\"photon_ptcone30\"][row_indices, sublead_pts]\n",
    "    macro_events[b\"photon_2lead_etcone20\"] = micro_events[b\"photon_etcone20\"][row_indices, sublead_pts]\n",
    "    \n",
    "    macro_filter = (  (macro_events[b\"photon_1lead_pt\"] > 25000)\n",
    "                    &\n",
    "                      (macro_events[b\"photon_2lead_pt\"] > 25000)\n",
    "                    &\n",
    "                      (macro_events[b\"photon_1lead_isTightID\"])\n",
    "                    &\n",
    "                      (macro_events[b\"photon_2lead_isTightID\"])\n",
    "                    &\n",
    "                      (macro_events[b\"photon_1lead_trigMatched\"])\n",
    "                    &\n",
    "                      (macro_events[b\"photon_2lead_trigMatched\"])\n",
    "                    & \n",
    "                      (macro_events[b\"photon_1lead_ptcone30\"] < 0.065)\n",
    "                    & \n",
    "                      (macro_events[b\"photon_1lead_etcone20\"] < 0.065)\n",
    "                    & \n",
    "                      (macro_events[b\"photon_2lead_ptcone30\"] < 0.065)\n",
    "                    & \n",
    "                      (macro_events[b\"photon_2lead_etcone20\"] < 0.065)\n",
    "                   )\n",
    "    \n",
    "    dict_apply_mask(macro_events, macro_filter)\n",
    "    macro_mask[macro_mask] = macro_filter\n",
    "\n",
    "    macro_events[b\"h_mass\"] = np.sqrt(2.*macro_events[b\"photon_1lead_E\"]\n",
    "                                        *macro_events[b\"photon_2lead_E\"]\n",
    "                                        *(1. - atlas_two_cosine(macro_events, b\"photon_1lead\", b\"photon_2lead\"))\n",
    "                                     )\n",
    "    \n",
    "    mass_cutoff =   (macro_events[b\"photon_1lead_E\"]/macro_events[b\"h_mass\"] > 0.35) \\\n",
    "                  & (macro_events[b\"photon_2lead_E\"]/macro_events[b\"h_mass\"] > 0.25) \\\n",
    "                  & (macro_events[b\"h_mass\"] >= 115000.) \\\n",
    "                  & (macro_events[b\"h_mass\"] <= 135000)\n",
    "    \n",
    "    dict_apply_mask(macro_events, mass_cutoff)\n",
    "    macro_mask[macro_mask] = mass_cutoff\n",
    "    \n",
    "    del macro_events[b\"photon_1lead_isTightID\"]\n",
    "    del macro_events[b\"photon_2lead_isTightID\"]\n",
    "    del macro_events[b\"photon_1lead_trigMatched\"]\n",
    "    del macro_events[b\"photon_2lead_trigMatched\"]\n",
    "    del macro_events[b\"photon_1lead_ptcone30\"]  # both null\n",
    "    del macro_events[b\"photon_2lead_ptcone30\"]\n",
    "    del macro_events[b\"trigP\"]\n",
    "    \n",
    "    return macro_events, macro_mask\n",
    "#process_photons(test_mc_events, np.ones_like(test_mc_events[b\"photon_n\"], dtype=np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_weights(events, mask):\n",
    "    total_weights = events[b\"SumWeights\"][0]\n",
    "    x_section = events[b\"XSection\"][0]\n",
    "    weights = (  events[b\"mcWeight\"]\n",
    "               * events[b'scaleFactor_PILEUP'] \n",
    "               * events[b'scaleFactor_ELE'] \n",
    "               * events[b'scaleFactor_MUON'] \n",
    "               * events[b'scaleFactor_PHOTON'] \n",
    "               * events[b'scaleFactor_TAU'] \n",
    "               * events[b'scaleFactor_BTAG'] \n",
    "               * events[b'scaleFactor_LepTRIGGER'] \n",
    "               * events[b'scaleFactor_PhotonTRIGGER']\n",
    "              )[mask]/total_weights*x_section\n",
    "    return {b\"weight\": weights}, mask\n",
    "#process_weights(test_mc_events, np.ones_like(test_mc_events[b\"photon_n\"], dtype=np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_descriptive(events, field):\n",
    "    values = events[field]\n",
    "    return {\n",
    "        field + b\"_min\": values.min(),\n",
    "        field + b\"_max\": values.max(),\n",
    "        field + b\"_mean\": values.mean(),\n",
    "        field + b\"_sum\": values.mean(),\n",
    "        field + b\"_std\": values.std()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_jets(events, mask):\n",
    "    macro_mask = mask.copy()\n",
    "    \n",
    "    macro_events = {}\n",
    "    micro_events = {}\n",
    "    \n",
    "    macro_events[b\"jet_n\"] = events[b\"jet_n\"][macro_mask]\n",
    "    \n",
    "    micro_events[b\"jet_pt\"] = events[b\"jet_pt\"][macro_mask]\n",
    "    micro_events[b\"jet_theta\"] = np.arctan(eta2tg_theta(events[b\"jet_eta\"][macro_mask]))\n",
    "    micro_events[b\"jet_phi\"] = events[b\"jet_phi\"][macro_mask]\n",
    "    micro_events[b\"jet_E\"] = events[b\"jet_E\"][macro_mask]\n",
    "    micro_events[b\"jet_MV2c10\"] = events[b\"jet_MV2c10\"][macro_mask]\n",
    "    \n",
    "    macro_events.update(extract_descriptive(micro_events, b\"jet_pt\"))\n",
    "    macro_events.update(extract_descriptive(micro_events, b\"jet_phi\"))\n",
    "    macro_events.update(extract_descriptive(micro_events, b\"jet_E\"))\n",
    "    macro_events.update(extract_descriptive(micro_events, b\"jet_theta\"))\n",
    "    macro_events.update(extract_descriptive(micro_events, b\"jet_MV2c10\"))\n",
    "    \n",
    "    return macro_events, macro_mask\n",
    "#process_jets(test_mc_events, np.ones_like(test_mc_events[b\"jet_n\"], dtype=np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lep(events, mask):\n",
    "    macro_mask = mask.copy()\n",
    "    \n",
    "    macro_events = {}\n",
    "    micro_events = {}\n",
    "    \n",
    "    macro_events[b\"lep_n\"] = events[b\"lep_n\"][macro_mask]\n",
    "    \n",
    "    micro_events[b\"lep_pt\"] = events[b\"lep_pt\"][macro_mask]\n",
    "    micro_events[b\"lep_theta\"] = np.arctan(eta2tg_theta(events[b\"lep_eta\"][macro_mask]))\n",
    "    micro_events[b\"lep_phi\"] = events[b\"lep_phi\"][macro_mask]\n",
    "    micro_events[b\"lep_E\"] = events[b\"lep_E\"][macro_mask]\n",
    "    micro_events[b\"lep_z0\"] = events[b\"lep_z0\"][macro_mask]\n",
    "    micro_events[b\"lep_charge\"] = events[b\"lep_charge\"][macro_mask]\n",
    "    micro_events[b\"lep_ptcone30\"] = events[b\"lep_ptcone30\"][macro_mask]\n",
    "    micro_events[b\"lep_etcone20\"] = events[b\"lep_etcone20\"][macro_mask]\n",
    "    \n",
    "    macro_events.update(extract_descriptive(micro_events, b\"lep_pt\"))\n",
    "    macro_events.update(extract_descriptive(micro_events, b\"lep_phi\"))\n",
    "    macro_events.update(extract_descriptive(micro_events, b\"lep_E\"))\n",
    "    macro_events.update(extract_descriptive(micro_events, b\"lep_theta\"))\n",
    "    macro_events.update(extract_descriptive(micro_events, b\"lep_charge\"))\n",
    "    macro_events.update(extract_descriptive(micro_events, b\"lep_z0\"))\n",
    "    macro_events.update(extract_descriptive(micro_events, b\"lep_ptcone30\"))\n",
    "    macro_events.update(extract_descriptive(micro_events, b\"lep_etcone20\"))\n",
    "    \n",
    "    return macro_events, macro_mask\n",
    "#process_lep(test_mc_events, np.ones_like(test_mc_events[b\"lep_n\"], dtype=np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_backprop(mask_seq, obj_seq):\n",
    "    mask_obj_iter = iter(zip(mask_seq[::-1], obj_seq[::-1]))\n",
    "    prev_mask, _ = next(mask_obj_iter)\n",
    "    for mask, obj in mask_obj_iter:\n",
    "        conditional_mask = (mask & prev_mask)[mask]\n",
    "        dict_apply_mask(obj, conditional_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_event_batch(events):\n",
    "    mask_seq = []\n",
    "    obj_seq = []\n",
    "    \n",
    "    mask = np.ones_like(events[b\"photon_n\"], dtype=np.bool)\n",
    "    \n",
    "    photons, mask = process_photons(events, mask)\n",
    "    mask_seq.append(mask)\n",
    "    obj_seq.append(photons)\n",
    "    \n",
    "    jets, mask = process_jets(events, mask)\n",
    "    mask_seq.append(mask)\n",
    "    obj_seq.append(photons)\n",
    "    \n",
    "    leptons, mask = process_lep(events, mask)\n",
    "    mask_seq.append(mask)\n",
    "    obj_seq.append(leptons)\n",
    "    \n",
    "    weights, mask = process_weights(events, mask)\n",
    "    mask_seq.append(mask)\n",
    "    obj_seq.append(weights)\n",
    "        \n",
    "    mask_backprop(mask_seq, obj_seq)\n",
    "    \n",
    "    other_features = {\n",
    "          b\"met_et\": events[b\"met_et\"]\n",
    "        , b\"met_phi\": events[b\"met_phi\"]\n",
    "    }\n",
    "    dict_apply_mask(other_features, mask)\n",
    "    \n",
    "    batch = {}\n",
    "    for obj in chain(obj_seq, [other_features]):\n",
    "        batch.update(obj)\n",
    "    return pd.DataFrame(batch)\n",
    "    \n",
    "#process_event_batch(test_events, bin_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def touch(path):\n",
    "    if os.path.exists(path):\n",
    "        return True\n",
    "    dirpath, filename = os.path.dirname(path), os.path.basename(path)\n",
    "    os.makedirs(dirpath, exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        f.flush()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(filepath, label, fout, flock, entrysteps, write_header):\n",
    "    print(filepath)\n",
    "    with uproot.open(filepath) as f:\n",
    "        for i, data in enumerate(f[\"mini\"].iterate([\"*\"], entrysteps=entrysteps)):\n",
    "            print(label, \"Processing: \" + str((i+1)*entrysteps) + \"\\n\")\n",
    "            processed_batch = process_event_batch(data)\n",
    "            processed_batch.columns = [q.decode(\"utf-8\") for q in processed_batch.columns]\n",
    "            processed_batch[\"label\"] = label\n",
    "            with flock:\n",
    "                processed_batch.to_csv(fout, sep=\"\\t\", header=bool(write_header.value), index=False)\n",
    "                if write_header.value:\n",
    "                    write_header.value = 0\n",
    "                fout.flush()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def events_per_file(datasets, output_file):\n",
    "    entrysteps = 100000\n",
    "    \n",
    "    processes = []\n",
    "    output_lock = Lock()\n",
    "    touch(output_file)\n",
    "    output_file_fd = open(output_file, \"a\")\n",
    "    write_header = Value(\"b\", 1)\n",
    "    \n",
    "    events = {}\n",
    "    for (process, thetype, name), meta, fullpath in yield_files_with_meta(datasets):\n",
    "        label = meta.get(\"tag\") or f\"{process}.{thetype}.{name}\"\n",
    "        p = Process(target=process_file, args=(fullpath, label, output_file_fd, output_lock, entrysteps, write_header))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "        \n",
    "    [p.join() for p in processes]\n",
    "    \n",
    "    output_file_fd.close()\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process:  GamGam\n",
      "Type:  MC\n",
      "File:  mc_341081.ttH125_gamgam.GamGam\n",
      "root://eosuser.cern.ch//eos/user/a/ananiev/data/GamGam/MC/mc_341081.ttH125_gamgam.GamGam.root\n",
      "File:  mc_343981.ggH125_gamgam.GamGam\n",
      "root://eosuser.cern.ch//eos/user/a/ananiev/data/GamGam/MC/mc_343981.ggH125_gamgam.GamGam.root\n",
      "File:  mc_345041.VBFH125_gamgam.GamGam\n",
      "root://eosuser.cern.ch//eos/user/a/ananiev/data/GamGam/MC/mc_345041.VBFH125_gamgam.GamGam.root\n",
      "File:  mc_345318.WpH125J_Wincl_gamgam.GamGam\n",
      "root://eosuser.cern.ch//eos/user/a/ananiev/data/GamGam/MC/mc_345318.WpH125J_Wincl_gamgam.GamGam.root\n",
      "File:  mc_345319.ZH125J_Zincl_gamgam.GamGam\n",
      "root://eosuser.cern.ch//eos/user/a/ananiev/data/GamGam/MC/mc_345319.ZH125J_Zincl_gamgam.GamGam.root\n",
      "Wp Processing: 100000\n",
      "\n",
      "Z Processing: 100000\n",
      "\n",
      "gg Processing: 100000\n",
      "\n",
      "Wp Processing: 200000\n",
      "\n",
      "VBF Processing: 100000\n",
      "\n",
      "Z Processing: 200000\n",
      "\n",
      "gg Processing: 200000\n",
      "\n",
      "Z Processing: 300000\n",
      "\n",
      "VBF Processing: 200000\n",
      "\n",
      "tt Processing: 100000\n",
      "\n",
      "gg Processing: 300000\n",
      "\n",
      "gg Processing: 400000\n",
      "\n",
      "VBF Processing: 300000\n",
      "\n",
      "VBF Processing: 400000\n",
      "\n",
      "tt Processing: 200000\n",
      "\n",
      "gg Processing: 500000\n",
      "\n",
      "VBF Processing: 500000\n",
      "\n",
      "gg Processing: 600000\n",
      "\n",
      "tt Processing: 300000\n",
      "\n",
      "tt Processing: 400000\n",
      "\n",
      "gg Processing: 700000\n",
      "\n",
      "tt Processing: 500000\n",
      "\n",
      "gg Processing: 800000\n",
      "\n",
      "gg Processing: 900000\n",
      "\n",
      "tt Processing: 600000\n",
      "\n",
      "gg Processing: 1000000\n",
      "\n",
      "gg Processing: 1100000\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "events_per_file(datasets, os.path.join(output_path, \"hgg_features.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
