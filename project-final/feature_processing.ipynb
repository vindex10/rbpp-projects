{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature processing for production mechanism classification\n",
    "\n",
    "Di-photon event selection based on https://arxiv.org/abs/1802.04146"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from itertools import chain\n",
    "from multiprocessing.dummy import Process\n",
    "from multiprocessing.dummy import Lock\n",
    "from multiprocessing.dummy import Value\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import uproot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "data_path = \"eosuser.cern.ch//eos/user/a/ananiev/data/\"\n",
    "output_path = \"/afs/cern.ch/user/a/ananiev/cernbox/output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still keep working with monte carlo simulations, since only they have labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"GamGam\": {\n",
    "        \"MC\": [\n",
    "            (\"mc_341081.ttH125_gamgam.GamGam\", {\"tag\": \"tt\"}),\n",
    "            (\"mc_343981.ggH125_gamgam.GamGam\", {\"tag\": \"gg\"}),\n",
    "            (\"mc_345041.VBFH125_gamgam.GamGam\", {\"tag\": \"VBF\"}),\n",
    "            (\"mc_345318.WpH125J_Wincl_gamgam.GamGam\", {\"tag\": \"Wp\"}),\n",
    "            (\"mc_345319.ZH125J_Zincl_gamgam.GamGam\", {\"tag\": \"Z\"})\n",
    "\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "test_mc = uproot.open(os.path.join(\"root://\", data_path, \"GamGam\", \"MC\", f\"{datasets['GamGam']['MC'][0][0]}.root\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "test_mc_events = next(test_mc[\"mini\"].iterate([\"*\"], entrystop=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def yield_files_with_meta(datasets):\n",
    "    for process_name, process in datasets.items():\n",
    "        print(\"Process: \", process_name)\n",
    "        for type_name, thetype in process.items():\n",
    "            print(\"Type: \", type_name)\n",
    "            for filedata in thetype:\n",
    "                try:\n",
    "                    filename, meta = filedata\n",
    "                except ValueError:\n",
    "                    filename = filedata\n",
    "                    meta = {}\n",
    "                print(\"File: \", filename)\n",
    "                fullpath = os.path.join(\"root://\", data_path, process_name, type_name, f\"{filename}.root\")\n",
    "                yield (process_name, type_name, filename), meta, fullpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def dict_apply_mask(d, mask, fields=None):\n",
    "    if fields is None:\n",
    "        fields = d.keys()\n",
    "    for f in fields:\n",
    "        d[f] = d[f][mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def eta2tg_theta(eta):\n",
    "    tg_theta = np.exp(-eta)\n",
    "    tg_theta = 2*tg_theta**2/(1 - tg_theta**2)\n",
    "    return tg_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def atlas_two_cosine(events, p1, p2):\n",
    "    tg_theta_1 = eta2tg_theta(events[p1+b\"_eta\"])\n",
    "    tg_theta_2 = eta2tg_theta(events[p2+b\"_eta\"])\n",
    "    cos_delta_phi = np.cos(events[p2+b\"_phi\"] - events[p1+b\"_phi\"])\n",
    "    return (cos_delta_phi + tg_theta_1*tg_theta_2)/np.sqrt((tg_theta_1**2 + 1)*(tg_theta_2**2 + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still follow \"macro\" / \"micro\" features classification, where \"micro\" features describe particles within the event (pT of 1st, 2nd, 3rd photon...). We extract features from event batch in groups per particle type. We begin from photons. Basically we repeat cuts applied in Higgs diphoton analysis, but here we use thinner region around higgs mass and also we don't care much about the rapidity cuts. Since we train on MC, we don't need to check whether events hit the detector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_photons(events, mask):\n",
    "    macro_mask = mask.copy()\n",
    "    \n",
    "    macro_events = {}\n",
    "    micro_events = {}\n",
    "    \n",
    "    macro_events[b\"photon_n\"] = events[b\"photon_n\"][macro_mask]\n",
    "    macro_events[b\"trigP\"] = events[b\"trigP\"][macro_mask]\n",
    "    n_threshold = macro_events[b\"photon_n\"] >= 2\n",
    "    is_diphoton = macro_events[b\"trigP\"]\n",
    "    macro_mask[macro_mask] = n_threshold & is_diphoton\n",
    "    \n",
    "    micro_events[b\"photon_pt\"] = events[b\"photon_pt\"][macro_mask]\n",
    "    micro_events[b\"photon_eta\"] = events[b\"photon_eta\"][macro_mask]\n",
    "    micro_events[b\"photon_phi\"] = events[b\"photon_phi\"][macro_mask]\n",
    "    micro_events[b\"photon_E\"] = events[b\"photon_E\"][macro_mask]\n",
    "    micro_events[b\"photon_isTightID\"] = events[b\"photon_isTightID\"][macro_mask]\n",
    "    micro_events[b\"photon_trigMatched\"] = events[b\"photon_trigMatched\"][macro_mask]\n",
    "    micro_events[b\"photon_ptcone30\"] = events[b\"photon_ptcone30\"][macro_mask]\n",
    "    micro_events[b\"photon_etcone20\"] = events[b\"photon_etcone20\"][macro_mask]\n",
    "    \n",
    "    \n",
    "    pts = micro_events[b\"photon_pt\"].argsort(ascending=False)\n",
    "    row_indices = np.arange(pts.shape[0])\n",
    "    lead_pts = pts[:, 0]\n",
    "    sublead_pts = pts[:, 1]\n",
    "    \n",
    "    \n",
    "    macro_events[b\"photon_n\"] = macro_events[b\"photon_n\"][macro_mask]\n",
    "    macro_events[b\"photon_1lead_pt\"] = micro_events[b\"photon_pt\"][row_indices, lead_pts]\n",
    "    macro_events[b\"photon_1lead_eta\"] = micro_events[b\"photon_eta\"][row_indices, lead_pts]\n",
    "    macro_events[b\"photon_1lead_phi\"] = micro_events[b\"photon_phi\"][row_indices, lead_pts]\n",
    "    macro_events[b\"photon_1lead_E\"] = micro_events[b\"photon_E\"][row_indices, lead_pts]\n",
    "    macro_events[b\"photon_1lead_isTightID\"] = micro_events[b\"photon_isTightID\"][row_indices, lead_pts]\n",
    "    macro_events[b\"photon_1lead_trigMatched\"] = micro_events[b\"photon_trigMatched\"][row_indices, lead_pts]\n",
    "    macro_events[b\"photon_1lead_ptcone30\"] = micro_events[b\"photon_ptcone30\"][row_indices, lead_pts]\n",
    "    macro_events[b\"photon_1lead_etcone20\"] = micro_events[b\"photon_etcone20\"][row_indices, lead_pts]\n",
    "    macro_events[b\"photon_2lead_pt\"] = micro_events[b\"photon_pt\"][row_indices, sublead_pts]\n",
    "    macro_events[b\"photon_2lead_eta\"] = micro_events[b\"photon_eta\"][row_indices, sublead_pts]\n",
    "    macro_events[b\"photon_2lead_phi\"] = micro_events[b\"photon_phi\"][row_indices, sublead_pts]\n",
    "    macro_events[b\"photon_2lead_E\"] = micro_events[b\"photon_E\"][row_indices, sublead_pts]\n",
    "    macro_events[b\"photon_2lead_isTightID\"] = micro_events[b\"photon_isTightID\"][row_indices, sublead_pts]\n",
    "    macro_events[b\"photon_2lead_trigMatched\"] = micro_events[b\"photon_trigMatched\"][row_indices, sublead_pts]\n",
    "    macro_events[b\"photon_2lead_ptcone30\"] = micro_events[b\"photon_ptcone30\"][row_indices, sublead_pts]\n",
    "    macro_events[b\"photon_2lead_etcone20\"] = micro_events[b\"photon_etcone20\"][row_indices, sublead_pts]\n",
    "    \n",
    "    macro_filter = (  (macro_events[b\"photon_1lead_pt\"] > 25000)\n",
    "                    &\n",
    "                      (macro_events[b\"photon_2lead_pt\"] > 25000)\n",
    "                    &\n",
    "                      (macro_events[b\"photon_1lead_isTightID\"])\n",
    "                    &\n",
    "                      (macro_events[b\"photon_2lead_isTightID\"])\n",
    "                    &\n",
    "                      (macro_events[b\"photon_1lead_trigMatched\"])\n",
    "                    &\n",
    "                      (macro_events[b\"photon_2lead_trigMatched\"])\n",
    "                    & \n",
    "                      (macro_events[b\"photon_1lead_ptcone30\"] < 0.065)\n",
    "                    & \n",
    "                      (macro_events[b\"photon_1lead_etcone20\"] < 0.065)\n",
    "                    & \n",
    "                      (macro_events[b\"photon_2lead_ptcone30\"] < 0.065)\n",
    "                    & \n",
    "                      (macro_events[b\"photon_2lead_etcone20\"] < 0.065)\n",
    "                   )\n",
    "    \n",
    "    dict_apply_mask(macro_events, macro_filter)\n",
    "    macro_mask[macro_mask] = macro_filter\n",
    "\n",
    "    macro_events[b\"h_mass\"] = np.sqrt(2.*macro_events[b\"photon_1lead_E\"]\n",
    "                                        *macro_events[b\"photon_2lead_E\"]\n",
    "                                        *(1. - atlas_two_cosine(macro_events, b\"photon_1lead\", b\"photon_2lead\"))\n",
    "                                     )\n",
    "    \n",
    "    mass_cutoff =   (macro_events[b\"photon_1lead_E\"]/macro_events[b\"h_mass\"] > 0.35) \\\n",
    "                  & (macro_events[b\"photon_2lead_E\"]/macro_events[b\"h_mass\"] > 0.25) \\\n",
    "                  & (macro_events[b\"h_mass\"] >= 115000.) \\\n",
    "                  & (macro_events[b\"h_mass\"] <= 135000)\n",
    "    \n",
    "    dict_apply_mask(macro_events, mass_cutoff)\n",
    "    macro_mask[macro_mask] = mass_cutoff\n",
    "    \n",
    "    del macro_events[b\"photon_1lead_isTightID\"]\n",
    "    del macro_events[b\"photon_2lead_isTightID\"]\n",
    "    del macro_events[b\"photon_1lead_trigMatched\"]\n",
    "    del macro_events[b\"photon_2lead_trigMatched\"]\n",
    "    del macro_events[b\"photon_1lead_ptcone30\"]  # both null\n",
    "    del macro_events[b\"photon_2lead_ptcone30\"]\n",
    "    del macro_events[b\"trigP\"]\n",
    "    \n",
    "    return macro_events, macro_mask\n",
    "#process_photons(test_mc_events, np.ones_like(test_mc_events[b\"photon_n\"], dtype=np.bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights extraction completely repeats the di-photon analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_weights(events, mask):\n",
    "    total_weights = events[b\"SumWeights\"][0]\n",
    "    x_section = events[b\"XSection\"][0]\n",
    "    weights = (  events[b\"mcWeight\"]\n",
    "               * events[b'scaleFactor_PILEUP'] \n",
    "               * events[b'scaleFactor_ELE'] \n",
    "               * events[b'scaleFactor_MUON'] \n",
    "               * events[b'scaleFactor_PHOTON'] \n",
    "               * events[b'scaleFactor_TAU'] \n",
    "               * events[b'scaleFactor_BTAG'] \n",
    "               * events[b'scaleFactor_LepTRIGGER'] \n",
    "               * events[b'scaleFactor_PhotonTRIGGER']\n",
    "              )[mask]/total_weights*x_section\n",
    "    return {b\"weight\": weights}, mask\n",
    "#process_weights(test_mc_events, np.ones_like(test_mc_events[b\"photon_n\"], dtype=np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_descriptive(events, field):\n",
    "    values = events[field]\n",
    "    return {\n",
    "        field + b\"_min\": values.min(),\n",
    "        field + b\"_max\": values.max(),\n",
    "        field + b\"_mean\": values.mean(),\n",
    "        field + b\"_sum\": values.sum(),\n",
    "        field + b\"_std\": values.std()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For jets and leptons, in addition to their macro features we also extract `descriptive` features, like: min, max, mean, sum, std computed on microfeatures. For instance: max pt of leptons within the event. In this way we avoid the problem of variable feature number, which then will become an issue for ML algorithms. There are several ways to tackle that problem. While we are following only one of them, knowing the possibilities might be helpful when developing more advanced solutions.\n",
    "\n",
    "Varible feature number handling options:\n",
    "\n",
    "* Feature aggregation (the one we follow due to its simplicity)\n",
    "* Data imputation. Assume event always has \"15\" leptons, fill in the gaps with mean or from the distribution if number of leptons < \"15\".\n",
    "* Set kernels, that allow computing \"distance\" between sets agnostically to the order of elements and allowing sets of different sizes to be compared. One of the famous ones — [Pyramid Match](http://jmlr.csail.mit.edu/papers/volume8/grauman07a/grauman07a.pdf), that compares overlaps between histograms built from feautre sets.\n",
    "* Neural nets that find embedding of sets of varible size in the fixed dimensional space. Example: [DeepSets](https://papers.nips.cc/paper/6931-deep-sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_jets(events, mask):\n",
    "    macro_mask = mask.copy()\n",
    "    \n",
    "    macro_events = {}\n",
    "    micro_events = {}\n",
    "    \n",
    "    macro_events[b\"jet_n\"] = events[b\"jet_n\"][macro_mask]\n",
    "    \n",
    "    micro_events[b\"jet_pt\"] = events[b\"jet_pt\"][macro_mask]\n",
    "    micro_events[b\"jet_theta\"] = np.arctan(eta2tg_theta(events[b\"jet_eta\"][macro_mask]))\n",
    "    micro_events[b\"jet_phi\"] = events[b\"jet_phi\"][macro_mask]\n",
    "    micro_events[b\"jet_E\"] = events[b\"jet_E\"][macro_mask]\n",
    "    micro_events[b\"jet_MV2c10\"] = events[b\"jet_MV2c10\"][macro_mask]\n",
    "    \n",
    "    macro_events.update(extract_descriptive(micro_events, b\"jet_pt\"))\n",
    "    macro_events.update(extract_descriptive(micro_events, b\"jet_phi\"))\n",
    "    macro_events.update(extract_descriptive(micro_events, b\"jet_E\"))\n",
    "    macro_events.update(extract_descriptive(micro_events, b\"jet_theta\"))\n",
    "    macro_events.update(extract_descriptive(micro_events, b\"jet_MV2c10\"))\n",
    "    \n",
    "    return macro_events, macro_mask\n",
    "#process_jets(test_mc_events, np.ones_like(test_mc_events[b\"jet_n\"], dtype=np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lep(events, mask):\n",
    "    macro_mask = mask.copy()\n",
    "    \n",
    "    macro_events = {}\n",
    "    micro_events = {}\n",
    "    \n",
    "    macro_events[b\"lep_n\"] = events[b\"lep_n\"][macro_mask]\n",
    "    \n",
    "    micro_events[b\"lep_pt\"] = events[b\"lep_pt\"][macro_mask]\n",
    "    micro_events[b\"lep_theta\"] = np.arctan(eta2tg_theta(events[b\"lep_eta\"][macro_mask]))\n",
    "    micro_events[b\"lep_phi\"] = events[b\"lep_phi\"][macro_mask]\n",
    "    micro_events[b\"lep_E\"] = events[b\"lep_E\"][macro_mask]\n",
    "    micro_events[b\"lep_z0\"] = events[b\"lep_z0\"][macro_mask]\n",
    "    micro_events[b\"lep_charge\"] = events[b\"lep_charge\"][macro_mask]\n",
    "    micro_events[b\"lep_ptcone30\"] = events[b\"lep_ptcone30\"][macro_mask]\n",
    "    micro_events[b\"lep_etcone20\"] = events[b\"lep_etcone20\"][macro_mask]\n",
    "    \n",
    "    macro_events.update(extract_descriptive(micro_events, b\"lep_pt\"))\n",
    "    macro_events.update(extract_descriptive(micro_events, b\"lep_phi\"))\n",
    "    macro_events.update(extract_descriptive(micro_events, b\"lep_E\"))\n",
    "    macro_events.update(extract_descriptive(micro_events, b\"lep_theta\"))\n",
    "    macro_events.update(extract_descriptive(micro_events, b\"lep_charge\"))\n",
    "    macro_events.update(extract_descriptive(micro_events, b\"lep_z0\"))\n",
    "    macro_events.update(extract_descriptive(micro_events, b\"lep_ptcone30\"))\n",
    "    macro_events.update(extract_descriptive(micro_events, b\"lep_etcone20\"))\n",
    "    \n",
    "    return macro_events, macro_mask\n",
    "#process_lep(test_mc_events, np.ones_like(test_mc_events[b\"lep_n\"], dtype=np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def mask_backprop(mask_seq, obj_seq):\n",
    "    mask_obj_iter = iter(zip(mask_seq[::-1], obj_seq[::-1]))\n",
    "    prev_mask, _ = next(mask_obj_iter)\n",
    "    for mask, obj in mask_obj_iter:\n",
    "        conditional_mask = (mask & prev_mask)[mask]\n",
    "        dict_apply_mask(obj, conditional_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def process_event_batch(events):\n",
    "    mask_seq = []\n",
    "    obj_seq = []\n",
    "    \n",
    "    mask = np.ones_like(events[b\"photon_n\"], dtype=np.bool)\n",
    "    \n",
    "    photons, mask = process_photons(events, mask)\n",
    "    mask_seq.append(mask)\n",
    "    obj_seq.append(photons)\n",
    "    \n",
    "    jets, mask = process_jets(events, mask)\n",
    "    mask_seq.append(mask)\n",
    "    obj_seq.append(photons)\n",
    "    \n",
    "    leptons, mask = process_lep(events, mask)\n",
    "    mask_seq.append(mask)\n",
    "    obj_seq.append(leptons)\n",
    "    \n",
    "    weights, mask = process_weights(events, mask)\n",
    "    mask_seq.append(mask)\n",
    "    obj_seq.append(weights)\n",
    "        \n",
    "    mask_backprop(mask_seq, obj_seq)\n",
    "    \n",
    "    other_features = {\n",
    "          b\"met_et\": events[b\"met_et\"]\n",
    "        , b\"met_phi\": events[b\"met_phi\"]\n",
    "    }\n",
    "    dict_apply_mask(other_features, mask)\n",
    "    \n",
    "    batch = {}\n",
    "    for obj in chain(obj_seq, [other_features]):\n",
    "        batch.update(obj)\n",
    "    return pd.DataFrame(batch)\n",
    "    \n",
    "#process_event_batch(test_events, bin_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def touch(path):\n",
    "    if os.path.exists(path):\n",
    "        return True\n",
    "    dirpath, filename = os.path.dirname(path), os.path.basename(path)\n",
    "    os.makedirs(dirpath, exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        f.flush()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def process_file(filepath, label, fout, flock, entrysteps, write_header):\n",
    "    print(filepath)\n",
    "    with uproot.open(filepath) as f:\n",
    "        for i, data in enumerate(f[\"mini\"].iterate([\"*\"], entrysteps=entrysteps)):\n",
    "            print(label, \"Processing: \" + str((i+1)*entrysteps) + \"\\n\")\n",
    "            processed_batch = process_event_batch(data)\n",
    "            processed_batch.columns = [q.decode(\"utf-8\") for q in processed_batch.columns]\n",
    "            processed_batch[\"label\"] = label\n",
    "            with flock:\n",
    "                processed_batch.to_csv(fout, sep=\"\\t\", header=bool(write_header.value), index=False)\n",
    "                if write_header.value:\n",
    "                    write_header.value = 0\n",
    "                fout.flush()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def events_per_file(datasets, output_file):\n",
    "    entrysteps = 100000\n",
    "    \n",
    "    processes = []\n",
    "    output_lock = Lock()\n",
    "    touch(output_file)\n",
    "    output_file_fd = open(output_file, \"a\")\n",
    "    write_header = Value(\"b\", 1)\n",
    "    \n",
    "    events = {}\n",
    "    time_start = time.perf_counter()\n",
    "    for (process, thetype, name), meta, fullpath in yield_files_with_meta(datasets):\n",
    "        label = meta.get(\"tag\") or f\"{process}.{thetype}.{name}\"\n",
    "        p = Process(target=process_file, args=(fullpath, label, output_file_fd, output_lock, entrysteps, write_header))\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "        \n",
    "    [p.join() for p in processes]\n",
    "    \n",
    "    output_file_fd.close()\n",
    "    time_now = time.perf_counter()\n",
    "    print(\"Done!\", \"Time spent: \", time_now - time_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we apply all the feature extraction functions mentioned above to event batches. We store outputs from all the MC files into the same output file with features. Production mechanism is now stored in the separate column as a textual label for each row. We spawn a thread per MC file, since most of the time take IO operations, threads don't introduce overhead to the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process:  GamGam\n",
      "Type:  MC\n",
      "File:  mc_341081.ttH125_gamgam.GamGam\n",
      "root://eosuser.cern.ch//eos/user/a/ananiev/data/GamGam/MC/mc_341081.ttH125_gamgam.GamGam.root\n",
      "File:  mc_343981.ggH125_gamgam.GamGam\n",
      "root://eosuser.cern.ch//eos/user/a/ananiev/data/GamGam/MC/mc_343981.ggH125_gamgam.GamGam.root\n",
      "File:  mc_345041.VBFH125_gamgam.GamGam\n",
      "root://eosuser.cern.ch//eos/user/a/ananiev/data/GamGam/MC/mc_345041.VBFH125_gamgam.GamGam.root\n",
      "File:  mc_345318.WpH125J_Wincl_gamgam.GamGam\n",
      "root://eosuser.cern.ch//eos/user/a/ananiev/data/GamGam/MC/mc_345318.WpH125J_Wincl_gamgam.GamGam.root\n",
      "File:  mc_345319.ZH125J_Zincl_gamgam.GamGam\n",
      "root://eosuser.cern.ch//eos/user/a/ananiev/data/GamGam/MC/mc_345319.ZH125J_Zincl_gamgam.GamGam.root\n",
      "Z Processing: 100000\n",
      "\n",
      "VBF Processing: 100000\n",
      "\n",
      "tt Processing: 100000\n",
      "\n",
      "gg Processing: 100000\n",
      "\n",
      "Wp Processing: 100000\n",
      "\n",
      "Z Processing: 200000\n",
      "\n",
      "Wp Processing: 200000\n",
      "\n",
      "VBF Processing: 200000\n",
      "\n",
      "tt Processing: 200000\n",
      "\n",
      "Z Processing: 300000\n",
      "\n",
      "gg Processing: 200000\n",
      "\n",
      "VBF Processing: 300000\n",
      "\n",
      "gg Processing: 300000\n",
      "\n",
      "tt Processing: 300000\n",
      "\n",
      "VBF Processing: 400000\n",
      "\n",
      "VBF Processing: 500000\n",
      "\n",
      "gg Processing: 400000\n",
      "\n",
      "tt Processing: 400000\n",
      "\n",
      "tt Processing: 500000\n",
      "\n",
      "gg Processing: 500000\n",
      "\n",
      "gg Processing: 600000\n",
      "\n",
      "tt Processing: 600000\n",
      "\n",
      "gg Processing: 700000\n",
      "\n",
      "gg Processing: 800000\n",
      "\n",
      "gg Processing: 900000\n",
      "\n",
      "gg Processing: 1000000\n",
      "\n",
      "gg Processing: 1100000\n",
      "\n",
      "Done! Time spent:  36.36667452001711\n"
     ]
    }
   ],
   "source": [
    "events_per_file(datasets, os.path.join(output_path, \"hgg_features.tsv\"))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
